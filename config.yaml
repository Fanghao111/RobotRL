# ===================================
# Push Task RL Configuration
# ===================================

# Environment settings
env:
  max_episode_steps: 200        # Maximum steps per episode
  fixed_ee_z: 0.1               # Fixed end-effector height (m)
  success_threshold: 0.05       # Distance threshold for success (m)
  
  # Object spawn range
  object_x_range: [0.3, 0.5]
  object_y_range: [-0.2, 0.2]
  
  # Target spawn range  
  target_x_range: [0.7, 0.9]
  target_y_range: [-0.3, 0.3]

# Reward settings
reward:
  target_progress_coef: 30.0    # Coefficient for moving object toward target
  ee_approach_coef: 10.0        # Coefficient for EE approaching object
  contact_threshold: 0.15       # Distance to consider "in contact" (m)
  contact_reward: 0.05          # Reward for maintaining contact
  success_bonus: 100.0          # Bonus for reaching target
  step_penalty: -0.01           # Penalty per step

# Training settings
training:
  # PPO hyperparameters (state mode)
  state:
    learning_rate: 0.0001
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.01
    net_arch_pi: [256, 256]
    net_arch_vf: [256, 256]
  
  # PPO hyperparameters (image mode)
  image:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0
  
  # General training settings
  default_timesteps: 1000000
  default_n_envs: 12
  progress_update_freq: 32768   # Progress bar update frequency

# Evaluation settings
evaluation:
  default_episodes: 20
  
  # Video settings
  video:
    enabled: false
    frame_skip: 1               # Capture every step (1 step = 0.1s physical time)
    fps: 10                     # 10 fps = 1:1 realtime (each step is 0.1s)
    width: 320
    height: 240
  
  # Plot settings
  plot:
    dpi: 150
    figsize: [10, 8]
