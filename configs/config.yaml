# ===================================
# Push Task RL Configuration
# ===================================

# Environment settings
env:
  max_episode_steps: 200        # Maximum steps per episode
  fixed_ee_z: 0.1               # Fixed end-effector height (m)
  success_threshold: 0.05       # Distance threshold for success (m)
  orientation_threshold: 3.14    # Orientation threshold for success (rad, ~180 degrees)

  # Object and target dimensions (half extents)
  object_half_extents: [0.06, 0.04, 0.05]   # Object box half size (x, y, z) in meters
  target_half_extents: [0.06, 0.04, 0.001] # Target visual half size (x, y, z) in meters

  # Object spawn range
  object_x_range: [0.3, 0.5]
  object_y_range: [-0.2, 0.2]

  # Target spawn range
  target_x_range: [0.7, 0.9]
  target_y_range: [-0.3, 0.3]

  # Fixed EE initial position (set to avoid collision with object)
  # Object x_range is [0.3, 0.5], so EE at x=0.15 is safely behind
  fixed_ee_initial_pos: [0.15, 0.0]  # [x, y] position for EE reset

# Reward settings (一阶段训练：同时学习位置和方向)
reward:
  # 进度奖励系数（增量式）
  position_progress_coef: 50.0     # 位置进度奖励系数（提高权重）
  orientation_progress_coef: 30.0  # 方向进度奖励系数（提高权重）
  coupling_coef: 40.0              # 耦合奖励系数（两者同时改进时的bonus，提高以鼓励协同）
  alignment_coef: 0.0              # 推动方向对齐奖励系数（取消）

  # EE接近物体的奖励
  ee_approach_coef: 5.0            # EE接近物体的奖励系数（降低，减少干扰）
  contact_threshold: 0.15          # 判定为"接触"的距离阈值 (m)
  first_contact_bonus: 15.0        # 首次接触一次性奖励（提高，补偿稀疏问题）

  # 成功奖励和步数惩罚
  success_bonus: 150.0             # 成功到达目标的奖励（提高）
  step_penalty: -0.01              # 每步惩罚（鼓励尽快完成）

# Training settings
training:
  # PPO hyperparameters (state mode)
  state:
    learning_rate: 0.0001
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.01
    net_arch_pi: [256, 256, 128]
    net_arch_vf: [256, 256, 128]

  # PPO hyperparameters (image mode)
  image:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0

  # General training settings
  default_timesteps: 1000000
  default_n_envs: 12
  progress_update_freq: 32768   # Progress bar update frequency

# Evaluation settings
evaluation:
  default_episodes: 20

  # Video settings
  video:
    enabled: false
    frame_skip: 1               # Capture every step (1 step = 0.1s physical time)
    fps: 10                     # 10 fps = 1:1 realtime (each step is 0.1s)
    width: 320
    height: 240

  # Plot settings
  plot:
    dpi: 150
    figsize: [10, 8]
